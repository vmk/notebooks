{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\">\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "<hr style=\"clear: both\">\n",
    "\n",
    "## 02 - The data\n",
    "\n",
    "Below are number of exercises in Python and Pyspark. Press Shift-Enter to execute the code. You can use code completion by using tab.\n",
    "\n",
    "In this notebook we will introduce the dataset we will use today and apply some Spark transformations. Finally, we will store preprocessed data that can be used in later notebooks.\n",
    "\n",
    " 1. The data\n",
    " 2. 'Munging' the data and counting words (again...)\n",
    " 3. Dataframes to the rescue\n",
    " 4. Storing preprocessed data for later use\n",
    "\n",
    "During the exercises you may want to refer to [The PySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html) for more information on possible transformations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data: 20 newsgroups\n",
    "\n",
    "For the exercises today we will use the 20 newsgroups data. This datasets contains around 20000 newsgroup postings from 20 different newsgroups. A description of the original dataset can be found [here](http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html). \n",
    "\n",
    "We have already converted the raw dataset into a more manageable format: each newsgroup posting has been converted to a JSON object and these objects have been stored together in one large gzipped file (only some common headers were preserved in this process). This file is available in your notebook environment. And can be easily loaded using the SparkContext: sc. Load the data as [text](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=textFile#pyspark.SparkContext) file and print the first element of the RDD (use [first](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.first)). Note that you do not need to worry about decompressing the file first and can read straight from the gzipped file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "jsonPostsRDD = sc.textFile(\"data/20newsgroups.labelled.json.gz\")\n",
    "\n",
    "# Print the first element of the RDD\n",
    "print jsonPostsRDD<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If all went well you should have seen that the first element is a JSON document. In the next few exercises we will use RDDs and transformations to first convert the json data to a more manageable format. Then we will select one newsgroup, count the words and find the top 20 senders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Next, we are going to convert the tweets into JSON format. This will return a dictionary where each key is a property of the JSON doc. The newsgroups posts are in a 'flat' JSON object, so there are no embedded JSON object. \n",
    "\n",
    "In the next cell the conversion takes place and the first post is shown. \n",
    "\n",
    "Just execute the cell, there's noting to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "parsedJsonPostsRDD = jsonPostsRDD.map(lambda x: json.loads(x))\n",
    "parsedPosts = parsedJsonPostsRDD.take(1)\n",
    "print json.dumps(parsedPosts, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to fields in the posts\n",
    "\n",
    "We have made some selections for you to show how to access fields in the posts.\n",
    "\n",
    "This is pure Python (using dictionaries), although the data is contained in an RDD. You probably see what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "postSelectionRDD = parsedJsonPostsRDD.map(lambda d: [d[\"label\"], d[\"sender\"], d[\"date\"], d[\"subject\"]])\n",
    "print postSelectionRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Spark it is important to keep track of what code is executed on workers, and what code on the driver. To move data to and from the driver to the workers is very expensive Given a large enough datasets - it might not even be possible to move the contents of an RDD to the driver.\n",
    "\n",
    "RDDs are distributed over workers and transformations define a sequence of RDDs. Never try to define an RDD inside an RDD and beware of what code is executed by the driver.\n",
    "\n",
    "Let's make a quick list of all properties in a post. We'll do it the wrong way first, by doing a map on the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print parsedJsonPostsRDD.map(lambda x: x.keys()).take(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code is very inefficient, since all tweets in the RDD are processed, and we end up with an RDD with all keys for all posts. It would be better to take a single post and then outside an RDD compute the keys. Note that then the computation of the keys is done by the driver.\n",
    "\n",
    "Try to do this in a single statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "print parsedJsonPostsRDD<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting properties, filtering on those properties, counting words and senders/posters\n",
    "\n",
    "In the next few exercises we will make a selection of a few properties, filter out a specific newsgroup and count the words and top 10 posters (senders).\n",
    "\n",
    "Start by creating an RDD with the following headers or properties : {label, from, subject, content}. Print the first element of the resulting RDD to verify your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "postsRDD = parsedJsonPostsRDD.map(<FILL IN>)\n",
    "print postsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering an RDD\n",
    "\n",
    "Besides map and reduce Spark has many convenient transformations on RDDs available out-of-the-box. In the next exercise use [filter](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.filter) to create an RDD that only consists of postings to the 'sci.space' newsgroup (do onsider the datatype - an array per post -  in the RDD you created in the previous exercise). Print the number of postings in the newsgroup using [count](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.count) and print the first post in the RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "scispacePostsRDD = postsRDD.filter(<FILL IN>)\n",
    "print scispacePostsRDD.count()\n",
    "print scispacePostsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words\n",
    "\n",
    "To do a wordcount on the newsgroup we first need to do a bit of cleaning up.\n",
    "\n",
    "Use a series of map operations to transform the content part of the RDD from the previous exercise. Instead of the content as a long string we want to end up with the content part as a list of tokens or words. Before creating that list encode the data as utf-8 and replace all non-words with single space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# ?\n",
    "tokenizedPostsRDD = (scispacePostsRDD<FILL IN>)\n",
    "print tokenizedPostsRDD.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will count the words in the newsgroup by again applying several transformations on the tokenizedPostsRDD:\n",
    "\n",
    "1. use a flatMap to create an RDD that consists of only the words from the posts\n",
    "2. use a filter to remove all words with a length shorter than 2 characters\n",
    "3. create (word, 1) tuples, lowercase each word\n",
    "4. use reduceByKey  to add the results for each word\n",
    "\n",
    "We will print the top ten words by making use of the [takeOrdered](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=takeordered#pyspark.RDD.takeOrdered) action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wcRDD = (tokenizedPostsRDD<FILL IN>)\n",
    "print wcRDD.takeOrdered(10, lambda x : -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting senders/posters\n",
    "\n",
    "Using similar operations as in the wordcount exercise, give a top-ten list of posters using the 'from' property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "fromCountRDD = (scispacePostsRDD<FILL IN>)\n",
    "print fromCountRDD.takeOrdered(10, lambda x : -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DataFrames to the rescue\n",
    "\n",
    "As you can see from the previous exercises transforming and converting data can be done quite easily using regular RDDs and transformations. There are howerever even more intuitive ways to deal with your data - provided it is in a structured format (i.e. it has a schema).\n",
    "\n",
    "From version Spark 1.0.0 onwards SparkSQL has been available in the Spark distribution. The SparkSQL module offers functions for loading and reading structured data, SQL syntax on the resulting loaded data and offers data organized in [DataFrames](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame). \n",
    "\n",
    "In the following steps we will load the JSON newsgroup data the SparkSQL way and explore the capabilities of this API.\n",
    "\n",
    "First, create an [SQLContext](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext) from the SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [SQLContext](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext) provides access to a [DataFrameReader](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader) which can directoy read from several structured formats or connectiions such as JSON, [Parquet](https://parquet.apache.org), JDBC and Hive.\n",
    "\n",
    "Load the json into a DataFrame using the SQLContext and print the schema of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = sqlCtx.read.json(\"data/20newsgroups.labelled.json.gz\")\n",
    "dataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame API provides a domain-specific language, alongside SQL, for manipulating structured data. It is, among others, possible to filter, select and group data.\n",
    "\n",
    "Grouping by the sender column and counting them is trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfGroups = dataFrame.groupBy(\"sender\").count()\n",
    "dfGroups.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering and selecting columns is equally simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfFilter = dataFrame.filter(dataFrame.label == \"sci.space\")\n",
    "dfSelect = dfFilter.select(\"label\", \"from\", \"subject\")\n",
    "dfSelect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are more comfortable with SQL the SparkSQL module allows you to use this as well. \n",
    "\n",
    "In order to talk SQL to a DataFrame it is needed to associate the DataFrame with a table name to use in SQL statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataFrame.registerTempTable('newsgroups')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use SQL statements to query the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = sqlCtx.sql(\"SELECT label, content FROM newsgroups\")\n",
    "labelsDistinct = sqlCtx.sql(\"SELECT DISTINCT(label) FROM newsgroups\")\n",
    "print sqlCtx.sql(\"SELECT COUNT(DISTINCT(label)) as labelCount FROM newsgroups\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list of the different newsgroups in the dataset, and broadcast this list to all executors so that it can be used in expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labelList = labelsDistinct.map(lambda r: r.label.encode('utf-8')).collect()\n",
    "sc.broadcast(labelList)\n",
    "from IPython.display import display, HTML\n",
    "th = \"<th>Label</th>\"\n",
    "td = [\"<tr><td>\" + d + \"</td></tr>\" for d in labelList]\n",
    "display(HTML(\"<table><thead><tr>\" + \"\".join(th) + \"</tr></thead><tbody>\" + \"\".join(td) + \"</tbody></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed 20 newsgroups. For machine learning it is probably convenient to have a numerical ID or label associated with each group. Let's create this ID and show a table of the mapping and the number of posts per newsgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = sqlCtx.sql(\"SELECT label FROM newsgroups\")\n",
    "labelCounts = labels.map(lambda r: (str(r.label), 1)).reduceByKey(lambda v1, v2: v1 + v2).collect()\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "th = \"<th>ID</th><th>Label</th><th>Messages</th>\"\n",
    "td = [\"<tr><td>\" + str(labelList.index(l)) +\"</td><td>\" + l + \"</td><td>\" + str(m) +\"</tr>\" for (l,m) in labelCounts]\n",
    "display(HTML(\"<table><thead><tr>\" + \"\".join(th) + \"</tr></thead><tbody>\" + \"\".join(td) + \"</tbody></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing data for later use\n",
    "\n",
    "In the final steps of this notebook we will make a subset of the data that is ready to use for subsequent notebooks;\n",
    "\n",
    "1. select only the following groups: {2,3,5,6,8,11,12,13,18,19}\n",
    "2. select only the following columns: {id, label, content}\n",
    "3. clean and tokenize the content column\n",
    "4. Safe the resulting dataframe into a Parquet file\n",
    "\n",
    "First select the relevant groups and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectedGroups = sqlCtx.sql(\"SELECT label, content FROM newsgroups WHERE label in ('sci.med', 'rec.autos', 'comp.windows.x', 'rec.sport.baseball', 'misc.forsale', 'talk.politics.misc', 'comp.graphics', 'alt.atheism', 'sci.space', 'talk.religion.misc')\")\n",
    "selectedGroups.show(5)\n",
    "print selectedGroups.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame can be accessed as an RDD of [Rows](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Row), although it is not simply a wrapper around RDD's. The way functions are applied to data using DataFrames differs from plain lambda's on RDDs; this becomes evident when working with functions such as the withColumn (this has to be a special kind of function: registered UDF's to be used in SQL).\n",
    "\n",
    "Here we will use this pattern to create a new RDD with an added column - the id. After adding the column we will chain a few extra maps to tokenize and clean the content field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "selectedGroupsWithID = (selectedGroups.withColumn(\"id\", selectedGroups.label)\n",
    "                        .map(lambda r: (r.label.encode(\"utf-8\"), re.sub('\\s+', ' ', r.content).strip(), labelList.index(r.label)))\n",
    "                        .map(lambda (l,c,i): (l, c.encode('utf-8').lower(), i))\n",
    "                        .map(lambda (l,c,i): (l, tokenizer.tokenize(c), i))\n",
    "                        )\n",
    "                                                     \n",
    "print selectedGroupsWithID.first()\n",
    "\n",
    "selectionDF = sqlCtx.createDataFrame(selectedGroupsWithID, [\"label\", \"content\", \"id\"])\n",
    "print \"\\n\"\n",
    "selectionDF.printSchema()\n",
    "print \"\\n\"\n",
    "selectionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, we will store the dataframe in an efficient format. For the columnar data in our dataframe [Parquet](https://parquet.apache.org) seems a good match. Fortunately the DataFrame API comes with [batteries included](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter.parquet) and storing our data is a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selectionDF.write.parquet(\"data/20newsgroups.selected.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
