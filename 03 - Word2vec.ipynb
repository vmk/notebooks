{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\">\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "<hr style=\"clear: both\">\n",
    "\n",
    "## 03 - Word2vec\n",
    "\n",
    "Below are number of exercises in Python and Pyspark. Press Shift-Enter to execute the code. You can use code completion by using tab.\n",
    "\n",
    "In this notebook we will use the subset of the 20newsgroup data created in the second notebook and train a word2vec model on the content of the posts. \n",
    "1. Loading the data\n",
    "2. Creating a model using ML Pipeline\n",
    "3. Creating a model using MLlib\n",
    "4. Using the model to create feature vectors\n",
    "5. ??? do something useful with the model\n",
    "\n",
    "During the exercises you may want to refer to the [PySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.htmlD) for more information on possible transformations and actions.\n",
    "\n",
    "First initialize Spark and SparkSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The data we need from 20newsgroups is stored in [Parquet](https://parquet.apache.org) - use SparkSQL to create a DataFrame from the stored data and print the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "postsDataFrame = sqlCtx.read.parquet(\"data/20newsgroups.selected.parquet\")\n",
    "postsDataFrame.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec using the ML Pipeline API\n",
    "\n",
    "As we are starting out with a dataframe it is very straightforward to train a word2vec model and apply it to the contents field using the [Spark ML API](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml). The Spark ML API is a set of high-level APIs built on top of DataFrames and much functionality of the MLlib packages is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "w2v = Word2Vec(inputCol=\"content\", outputCol=\"wvector\").setSeed(123456)\n",
    "\n",
    "# Create the model based on our \"content\" data\n",
    "model = w2v.fit(postsDataFrame)\n",
    "\n",
    "# Apply the model to our \"content\" data to create a vector for each newsgroup post in our set\n",
    "result = model.transform(postsDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's inspect a few features - note that these are of type [DenseVector](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector) - one per post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for feature in result.select(\"label\", \"wvector\").take(3):\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the schema we can see that the Pipeline has added an extra column to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can of course use the model to find synonyms for a given term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syms = model.findSynonyms(\"san\", 10)\n",
    "syms.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word2vec using MLlib\n",
    "\n",
    "Word2vec using MLlib is a little bit more involved. Let's do this in the form of an exercise.\n",
    "\n",
    "Start by creating an RDD that consists of only the token arrays from the postsDataFrame. Filter out any posts of length zero and print the count and two elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Select only the token arrays with length > 0\n",
    "tokenizedPosts = postsDataFrame<FILL IN>\n",
    "print tokenizedPosts.take(2)\n",
    "print tokenizedPosts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, initialize a word2vec model and fit the tokens. See the [word2vec documentation](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.feature.Word2Vec) for more details. In order to be able to compare the MLlib and ML Pipelines models use the same seed as the ML Pipelines example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "\n",
    "word2vec = <FILL IN>\n",
    "modelML = word2vec<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, print some synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syms = modelML.findSynonyms(\"san\", 10)\n",
    "for s in syms:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would now be tempting to use the transform function of the MLlib model to transform the words in our posts to feature vectors. Unfortunately this is not possible - see the [documentation](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.feature.Word2VecModel.transform) and note the \"Local use only\".\n",
    "\n",
    "There is a way however. We can get the vectors for each word from the model and broadcast these to all executors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelVectors = modelML.getVectors()\n",
    "vocabsize = len(modelVectors)\n",
    "any_word = \"san\"\n",
    "tmp_list = modelML.findSynonyms(any_word, vocabsize-1) \n",
    "\n",
    "list_words = [a[0] for a in tmp_list]\n",
    "list_words.append(any_word)\n",
    "\n",
    "nfeatures = modelML.transform(any_word).array.shape[0]\n",
    "nwords = len(list_words)\n",
    "\n",
    "print \"Type of the modelVectors:\", type(modelVectors)\n",
    "print \"Number of words in the model:\", nwords\n",
    "print \"Number of features per word: \", nfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vectors are in a Java format. Before being able to broadcast we will need to convert them to Python. Note that this operation can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyModelVectors = { w: list(modelVectors[w]) for w in modelVectors }\n",
    "\n",
    "print modelML.transform(\"san\")\n",
    "print pyModelVectors[\"san\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast the Pythonized vectors of the model so each executor has access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.broadcast(pyModelVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the broadcast vectors to transform the newsgroupdata. A utility function has been given that, from all the word vectors in a list will construct a feature. Take 3 of the posts and compare the output vectors to the ML Pipelines output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def transformWordList(wordList, w2Vectors, numFeatures):\n",
    "    numWords = len(wordList)\n",
    "    wvecs=[]\n",
    "    for w in wordList:\n",
    "        wvecs.append(w2Vectors.get(w, [0] * numFeatures))\n",
    "    swvecs = np.sum(wvecs, axis=0)\n",
    "    return [float(d)/float(numWords) for d in swvecs]\n",
    "\n",
    "postRDD = postsDataFrame.map(lambda r: (r.id, r.label, r.content)).filter(lambda (i,l,c): len(c) > 0)\n",
    "transformedPostRDD = postRDD<FILL IN>\n",
    "\n",
    "# Print 3 posts\n",
    "print <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ??? do something interesting with the model.\n",
    "\n",
    "Now it is up to you to do something interesting with the data and model. Good luck, have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
