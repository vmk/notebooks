{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://spark.apache.org/docs/latest/api/python/_static/spark-logo-hd.png)\n",
    "<img style=\"float: right\" src=\"https://e-infra.surfsara.nl/img/logo.png\">\n",
    "## 04 - Naive Bayes\n",
    "\n",
    "Below are number of exercises in Python and Pyspark. Press Shift-Enter to execute the code. You can use code completion by using tab.\n",
    "\n",
    "In this notebook we will use the subset of the 20newsgroup data created in the second notebook and train a Naive Bayes classifier on the content of the posts. \n",
    "1. Load the data\n",
    "2. Create a classifier using ML Pipeline and or MLlib, classify some test data and evaluatethe performance of your classifier\n",
    "\n",
    "During the exercises you may want to refer to [The PySpark documentation](https://spark.apache.org/docs/1.3.1/api/python/pyspark.htmlD) for more information on possible transformations and actions.\n",
    "\n",
    "First initialize Spark and SparkSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx = SQLContext(sc) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The data we need from 20newsgroups is stored in [Parquet](https://parquet.apache.org) - use SparkSQL to create a DataFrame from the stored data and print the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "postsDataFrame = sqlCtx.read.parquet(\"20newsgroups.selected.parquet\")\n",
    "postsDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create a classifier\n",
    "\n",
    "Next you will have to convert the posts to features vectors to be used by Naive Bayes. Spark offers several options such as term frequency or tf-idf. Take a look at the [ML](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature) and [MLlib](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.feature) feature packages. Both tf (using hashing) and tf-idf are available.\n",
    "\n",
    "Once you have features you can progress and train a classifier. You can use the [randomSplit](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) function on RDDs to create random subsets of the data for e.g. training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: build feature vectors, split the data, build a classifier, evaluate the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
