{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left\" src=\"images/spark.png\">\n",
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "<hr style=\"clear: both\">\n",
    "\n",
    "## 01 - The basics\n",
    "\n",
    "Below are number of exercises in Python and Pyspark. Press Shift-Enter to execute the code. You can use code completion by using tab.\n",
    "\n",
    "In this notebook we will start with the basics:\n",
    "\n",
    " 1. Working with lambda functions and the map and reduce functions in Python\n",
    " 2. Creating Spark RDDs and apply functions to 'distributed' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda functions\n",
    "\n",
    "Both map Reduce and Spark have taken their inspiration from functional programming and hence understanding lambda functions will help you to understand MapReduce and Spark. \n",
    "\n",
    "Lambda functions are anonymous functions, that is functions without a name. The term <b>lambda</b> simply denotes that a function is defined. What follows is a function statement, a single statement only, and no return statement. The essence of Spark and MapReduce is applying functions to distributed datasets. \n",
    "\n",
    "Finally, a lambda function can be assigned to a variable, which then is used as the name of the function. This is not possible with functions that are defined using <b>def</b>.\n",
    "\n",
    "Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This lambda function has two arguments x and y which are multiplied\n",
    "#  Note that there is no return statement and that the function is assigned to the variable l_times\n",
    "#  The : separates the arguments from the body of the function\n",
    "\n",
    "l_times = lambda x,y: x * y\n",
    "\n",
    "# Next we call the function by using the variable as a function\n",
    "result = l_times(2,3)\n",
    "print str(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda functions - exercise\n",
    "\n",
    "Next, define a lambda function which adds two numbers. Then execute the function on the integers 7 and 9 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# A lambda function to add two numbers\n",
    "my_add = <FILL IN>\n",
    "\n",
    "# add two numbers using the function just defined\n",
    "result = my_add(7, 9)\n",
    "\n",
    "print str(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce\n",
    "\n",
    "Functions are very important to both Map reduce and Spark. To see why let us look at a function called map, which is part of python.\n",
    "\n",
    "<b>map</b> is a function which takes two arguments, a function and a list. Map applies the function (it's first argument) to every element of the list (it's second argument).\n",
    "\n",
    "In the next cell we show how this works.\n",
    "We define a list, called <b>celsius</b> which contains a number of temperature measures in degrees Celsius. We are going to convert this list to degrees Fahrenheit.\n",
    "\n",
    "For this we write a function which will convert a single degree Celsius into Fahrenheit.  We then call <b>map</b> to apply this function to all elements of the list <b>celsius</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A list of temperature measures in degrees Celsius\n",
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "\n",
    "# A lambda function defining the conversion from a degree in Celsius to one in Fahrenheit\n",
    "convert = lambda x: (float(9)/5)*x + 32\n",
    "\n",
    "# by using map we apply the function to every element of the list celsius\n",
    "fahrenheit = map(convert, celsius)\n",
    "\n",
    "# We can do exactly the same by using the lambda expression inside the map statement directly:\n",
    "# fahrenheit = map(lambda x: (float(9)/5)*x + 32, celsius)\n",
    "print fahrenheit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have to use lambda functions here. We can define the convert function using <b>def</b> and use the name of the function as the first argument for map. Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# we define the same function as in the previous cell, now using def\n",
    "\n",
    "def convert_def(x):\n",
    "    fahr = (float(9)/5)*x + 32\n",
    "    return fahr\n",
    "\n",
    "# let's use it in map on the celsius list\n",
    "fahrenheit = map(<FILL IN>, celsius)\n",
    "print fahrenheit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is conceptually very similar to Hadoop's Map as in MapReduce. Except, the Python version is not executed in parallel. In Hadoop the functions, or programs, are run in parallel by many workers on different parts of the data.\n",
    "\n",
    "Next, let us look at Reduce. Python also has a function called <b>reduce</b> which takes as it's first argument a function, and as a second argument a list. The function has to have two arguments.\n",
    "\n",
    "<b>reduce</b> will then apply the function to the first two elements of the list and use this result together with the next item in the list to compute the next step. This procedure is repeated until the entire list is traversed.\n",
    "\n",
    "For example, suppose we want to add up all elements of the list [47, 11, 42, 13] then we write a function which will add up two integers, and we will call <b>reduce</b>. The <b>reduce</b> function will then proceed as depicted in the below:\n",
    "\n",
    "![python reduce](http://www.python-course.eu/images/reduce_diagram.png)\n",
    "\n",
    "Note that this function <b>reduce</b> resembles Hadoop's Reduce. Note, that in Hadoop you have to supply the Mappers and Reducers with a program, in python it is a function, which basically is the same thing.\n",
    "\n",
    "Spark works similarly. You have to write functions that you have feed into other functions, which will process data for you. Typically these functions will be applied in parallel to data that is distributed over the Spark executors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce - exercise\n",
    "\n",
    "As a final exercise, before we move to Spark, we are going to compute the mean of the fahrenheit list, using reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# First write a lambda function which adds up two elements\n",
    "\n",
    "sum_up = <FILL IN>\n",
    "# Use reduce to sum up the elements in fahrenheit\n",
    "total = reduce(sum_up, fahrenheit)\n",
    "\n",
    "# divide total by the length of the list fahrenheit\n",
    "# Use the division operator /\n",
    "mean = total / len(fahrenheit)\n",
    "print mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to test if you have the right value for mean\n",
    "from test_helper import Test\n",
    "import numpy as np\n",
    "\n",
    "Test.assertTrue(np.allclose(mean, 99.86), 'Wrong value for mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "\n",
    "Having these basics out of the way, we can move on to Spark. \n",
    "\n",
    "During the exercises you may want to refer to [The PySpark documentation](https://spark.apache.org/docs/1.3.1/api/python/pyspark.html) for more information on possible transformations and actions.\n",
    "\n",
    "Let us first create a simple RDD, based on a list of words. We will be using two partitions here for the RDD. We will use a SparkContext sc that has already been created for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordsList = ['Dog', 'Cat', 'Rabbit', 'Hare', 'Deer', 'Gull', 'Woodpecker', 'Mole']\n",
    "wordsRDD = sc.parallelize(wordsList, 2)\n",
    "# Print out the type of wordsRDD\n",
    "print type(wordsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map transformation\n",
    "\n",
    "We now want to change all words in the wordsRDD to their plural form. We will do this using a [map](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.map) transformation.\n",
    "Remember that the map function will apply the function to each element of the RDD. \n",
    "\n",
    "First, we will write a simple function that takes a single word as argument and return the word with an 's' added to it. In the next step we will use this function in a map transformation of the wordsRDD.\n",
    "\n",
    "Take a look at the function below and fill in the code at the tag &lt;FILL IN&gt;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def makePlural(word):\n",
    "    \"\"\"Adds an 's' to `word`.\n",
    "\n",
    "    Note:\n",
    "        This is a simple function that only adds an 's'.  \n",
    "\n",
    "    Args:\n",
    "        word (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "print makePlural('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the makePlural function as input for the map transformation on wordsRDD.\n",
    "The action [collect](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=collect#pyspark.RDD.collect) tranfers the content of the RDD to the driver. Note, that a large RDD may be scattered over many machines. In such a case a collect may not be a good idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "pluralRDD = wordsRDD.map(<FILL IN>)\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait a minute! We can actually achieve the same functionality by using lambda functions. In this case we define makePlural as a lambda function. \n",
    "\n",
    "Hint: The map function needs a lambda function as argument. This function needs one argument, let's call that x. The body of the function adds an 's' to the end of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A lambda function for adding s at the end of a string\n",
    "lambdaPluralRDD = wordsRDD.map(lambda x : x + 's')\n",
    "print lambdaPluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's do another map transformation. For each word in wordRDD determine its length. The Python function len(s) will return the length for a string s.\n",
    "â€‹\n",
    "You can do this with a lambda function, but there is another way... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordLengths = (wordsRDD.map(<FILL IN>)\n",
    "                 .collect())\n",
    "print wordLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your solution by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Length of each word \n",
    "from test_helper import Test\n",
    "   \n",
    "Test.assertEquals(sorted(wordLengths), sorted([3, 3, 6, 4, 4, 4, 10, 4]),\n",
    "                  'incorrect values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting words with key-value pairs\n",
    "\n",
    "Counting words in parallel is the 'hello world' in Spark and MapReduce - it's simple to understands but captures the essence of a lot of the concepts used in distributed computing with Spark or Hadoop MapReduce. \n",
    "\n",
    "In order to count words in parallel we are going to use an RDD which consist of simple key value pairs. We will call this RDD wordPairs and it will be result of a transformation of wordsRDD. For every word wordsRDD we want to have a (word, 1) tuple. Please fill in the code in the next cell at the place indicated and run the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordsRDD2 = sc.parallelize(\"baa baa black sheep have you any wool yes sir yes sir three bags full\".split())\n",
    "\n",
    "wordPairs = wordsRDD2.map(<FILL IN>)\n",
    "\n",
    "print wordPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your solution by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Pair RDDs\n",
    "Test.assertEquals(sorted(wordPairs.collect()),\n",
    "                  [('any', 1), ('baa', 1), ('baa', 1), ('bags', 1), ('black', 1), ('full', 1), ('have', 1), ('sheep', 1), ('sir', 1), ('sir', 1), ('three', 1), ('wool', 1), ('yes', 1), ('yes', 1), ('you', 1)],\n",
    "                  'incorrect value for wordPairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with, and creating tuples is central to using Spark. The creation of the (word, 1) in the exercise above works nicely if your input is already a list of tokens. How would you do this for a list of sentences, or documents? \n",
    "\n",
    "In fact, it is very common to want to map a function on an input list that returns multiple values in a list, but then not to want the output nested in the same way as the input was. As it is commonly needed Spark includes a [flatMap](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=map#pyspark.RDD.flatMap) transformation that will flatten the output of the function.\n",
    "\n",
    "First convert the sentenceRDD to an RDD with words, then convert this RDD to wordpairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "sentenceRDD = sc.parallelize([\"baa baa black sheep\", \"have you any wool\", \"yes sir yes sir\",  \"three bags full\"])\n",
    "wordsRDD3 = sentenceRDD.flatMap(<FILL IN>)\n",
    "wordPairs = wordsRDD3.map(<FILL IN>)\n",
    "\n",
    "print wordPairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Pair RDDs \n",
    "Test.assertEquals(sorted(wordPairs.collect()),\n",
    "                  [('any', 1), ('baa', 1), ('baa', 1), ('bags', 1), ('black', 1), ('full', 1), ('have', 1), ('sheep', 1), ('sir', 1), ('sir', 1), ('three', 1), ('wool', 1), ('yes', 1), ('yes', 1), ('you', 1)],\n",
    "                  'incorrect value for wordPairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to count all words by using [reduceByKey](https://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=reducebykey#pyspark.RDD.reduceByKey).\n",
    "\n",
    "reducebyKey expects the RDD to consist of key value pairs an it will perform a reduce operation per key. \n",
    "It will need a two-argument function as input that will work on the values only. Remember that a reduce function needs two arguments and will reduce all elements of the RDD to a single value.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "# The function that is input to reduceByKey only works on the values. Spark will execute this function per key\n",
    "wordCounts = wordPairs.reduceByKey(<FILL IN>)\n",
    "print wordCounts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TEST Counting using reduceByKey\n",
    "Test.assertEquals(sorted(wordCounts.collect()), \n",
    "                  [('any', 1), ('baa', 2), ('bags', 1), ('black', 1), ('full', 1), ('have', 1), ('sheep', 1), ('sir', 2), ('three', 1), ('wool', 1), ('yes', 2), ('you', 1)],\n",
    "                  'incorrect value for wordCounts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
